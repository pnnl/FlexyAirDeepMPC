"""
HW in the loop setup

for k in range(nsim):
    y, r, xmin, xmax = measurements()
    x0 = estimator(y)
    u = policy(x0,r,d,xmin,xmax)
    send_control(u[0])

"""

# python base imports
import argparse
import dill
import warnings

# machine learning data science imports
import numpy as np
import torch
# local imports
from neuromancer.plot import pltCL, pltOL
from neuromancer.datasets import FileDataset
import psl


def parse():
    parser = argparse.ArgumentParser()
    parser.add_argument('-ref_type', type=str, default='periodic', choices=['steps', 'periodic'],
                        help="shape of the reference signal")
    parser.add_argument('-dynamic_constraints', type=int, default=0, choices=[0, 1])
    return parser


class Simulator:
    def __init__(self, estimator=None, dynamics=None):
        self.estim = estimator
        self.dynamics = dynamics
        self.y = torch.ones(1, 1, 1)
        self.x = torch.zeros(1, self. dynamics.nx)

    def send_control(self, u, d, Y, x=None):
        estim_out = self.estim({'Yp': Y})
        inputs = {'x0_estim': estim_out['x0_estim'], 'U_pred_policy': u, 'Df': d,'Yf': Y[-1:]}
        outputs = self.dynamics(inputs)
        self.y = outputs['Y_pred_dynamics']
        self.x = outputs['X_pred_dynamics']

    def get_state(self):
        return self.y, self.x


class SimulatorX:
    def __init__(self, estimator=None, dynamics=None):
        self.estim = estimator
        self.dynamics = dynamics
        self.y = torch.ones(1, 1, 1)
        self.x = torch.zeros(1, self. dynamics.nx)

    def send_control(self, u, d, Y, x):
        inputs = {'x0_estim': x, 'U_pred_policy': u, 'Df': d,'Yf': Y[-1:]}
        outputs = self.dynamics(inputs)
        self.y = outputs['Y_pred_dynamics']
        self.x = outputs['X_pred_dynamics'].squeeze(0)

    def get_state(self):
        return self.y, self.x



def normalize(M, Mmin=None, Mmax=None):
        """
        :param M: (2-d np.array) Data to be normalized
        :param Mmin: (int) Optional minimum. If not provided is inferred from data.
        :param Mmax: (int) Optional maximum. If not provided is inferred from data.
        :return: (2-d np.array) Min-max normalized data
        """
        Mmin = M.min(axis=0).reshape(1, -1) if Mmin is None else Mmin
        Mmax = M.max(axis=0).reshape(1, -1) if Mmax is None else Mmax
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            M_norm = (M - Mmin) / (Mmax - Mmin)
        return np.nan_to_num(M_norm), Mmin.squeeze(), Mmax.squeeze()


def min_max_denorm(M, Mmin, Mmax):
    """
    denormalize min max norm
    :param M: (2-d np.array) Data to be normalized
    :param Mmin: (int) Minimum value
    :param Mmax: (int) Maximum value
    :return: (2-d np.array) Un-normalized data
    """
    M_denorm = M*(Mmax - Mmin) + Mmin
    return np.nan_to_num(M_denorm)


if __name__ == '__main__':
    # trained model with estimator
    args = parse().parse_args()

    # USE min_max_denorm and normalization values to denormalize signals
    #  y - ball position
    #  u - deepMPC control action, reference signal for PID
    #  d - fan speed, control action generated by PID
    normalizations = {'Ymin': -1.3061713953490333, 'Ymax': 32.77003662201578,
                      'Umin': -2.1711117, 'Umax': 33.45899931,
                      'Dmin': 29.46308055, 'Dmax': 48.97325791}

    model = 'model4'  # choose model, choices ['model0', 'model1', 'model2', 'model3', 'model4']

    if model == 'model0':
        # state feedback policy with estimator in the loop
        device_simulator = torch.load('../datasets/Flexy_air/device_test_models/model0/best_model_flexy1.pth', pickle_module=dill)
        policy_problem = torch.load('../datasets/Flexy_air/device_test_models/model0/best_model_flexy1_policy1.pth', pickle_module=dill)
        estimator = policy_problem.components[0]
        estimator.input_keys[0] = 'Yp'
        dynamics = device_simulator.components[1]
        dynamics.input_keys[2] = 'U_pred_policy'
        policy = policy_problem.components[1]
    else:
        # output feedback policy
        # model4 - policy with adaptive constraints
        dynamics = torch.load('../datasets/Flexy_air/device_test_models/'+model+'/best_dynamics_flexy.pth',
                                      pickle_module=dill)
        policy = torch.load('../datasets/Flexy_air/device_test_models/'+model+'/best_policy_flexy.pth',
                                    pickle_module=dill)
        estimator = torch.load('../datasets/Flexy_air/device_test_models/' + model + '/best_estimator_flexy.pth',
                            pickle_module=dill)
        estimator.input_keys[0] = 'Yp'
        policy.input_keys[0] = 'Yp'

    HW_emulator = Simulator(estimator=estimator, dynamics=dynamics)

    # dataset
    nsim = 3000
    dataset = FileDataset(system='flexy_air', nsim=nsim, norm=['U', 'D', 'Y'], nsteps=estimator.nsteps)
    ny = 1
    nu = 1
    nsteps = policy.nsteps

    R = {'steps': psl.Steps(nx=1, nsim=nsim, randsteps=30, xmax=0.7, xmin=0.3),
         'periodic': psl.Periodic(nx=1, nsim=nsim, numPeriods=20, xmax=0.7, xmin=0.3)}[args.ref_type]

    if args.dynamic_constraints:
        bounds_reference = {'Y_max': psl.Periodic(nx=ny, nsim=nsim, numPeriods=30, xmax=0.9, xmin=0.6),
                            'Y_min': psl.Periodic(nx=ny, nsim=nsim, numPeriods=25, xmax=0.4, xmin=0.1),
                            'U_max': np.ones([nsim, nu]), 'U_min': np.zeros([nsim, nu]), 'R': R}
    else:
        bounds_reference = {'Y_max': 0.8 * np.ones([nsim, ny]), 'Y_min': 0.2 * np.ones([nsim, ny]),
                            'U_max': np.ones([nsim, nu]), 'U_min': np.zeros([nsim, nu]), 'R': R}

    # Open loop
    yN = torch.zeros(nsteps, 1, 1)
    Y, U = [], []
    for k in range(nsim-nsteps):
        y, x = HW_emulator.get_state()
        yN = torch.cat([yN, y])[1:]
        d = torch.tensor(dataset.data['D'][k]).reshape(1, 1, -1).float()
        u = torch.tensor(dataset.data['U'][k]).reshape(1,1,-1).float()
        HW_emulator.send_control(u, d=d, Y=yN, x=x)
        U.append(u.detach().numpy().reshape(-1))
        Y.append(y.detach().numpy().reshape(-1))
    pltCL(Y=np.asarray(Y), R=dataset.data['Y'][:,:1], U=np.asarray(U))

    # Closed loop
    yN = torch.zeros(nsteps, 1, 1)
    Y, U, R = [], [], []
    Ymin, Ymax, Umin, Umax = [], [], [], []
    for k in range(nsim-nsteps):
        y, x = HW_emulator.get_state()
        yN = torch.cat([yN, y])[1:]
        estim_out = estimator({'Yp': yN})
        features = {'x0_estim': estim_out['x0_estim'], 'Yp': yN,
                    'Y_minf': torch.tensor(bounds_reference['Y_min'][k:nsteps + k]).float().reshape(nsteps, 1, -1),
                    'Y_maxf': torch.tensor(bounds_reference['Y_max'][k:nsteps + k]).float().reshape(nsteps, 1, -1),
                    'Rf': torch.tensor(bounds_reference['R'][k:nsteps+k]).float().reshape(nsteps,1,-1),
                    'Df': torch.tensor(dataset.data['D'][k:nsteps+k]).float().reshape(nsteps,1,-1)}
        policy_out = policy(features)
        uopt = policy_out['U_pred_policy'][0].reshape(1,1,-1).float()
        d = torch.tensor(dataset.data['D'][k]).reshape(1,1,-1).float()
        HW_emulator.send_control(uopt, d=d, Y=yN, x=x)
        U.append(uopt.detach().numpy().reshape(-1))
        Y.append(y.detach().numpy().reshape(-1))
        R.append(bounds_reference['R'][k])
        Ymax.append(bounds_reference['Y_max'][k])
        Ymin.append(bounds_reference['Y_min'][k])
        Umax.append(bounds_reference['U_max'][k])
        Umin.append(bounds_reference['U_min'][k])
    pltCL(Y=np.asarray(Y), R=np.asarray(R), U=np.asarray(U),
          Ymin=np.asarray(Ymin), Ymax=np.asarray(Ymax),
          Umin=np.asarray(Umin), Umax=np.asarray(Umax))